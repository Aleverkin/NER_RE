{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a506eb",
   "metadata": {},
   "source": [
    "Оцениваение: минимально в 2 раза больше обычного дз (если сделаете совсем много, возможны дополнительные бонусы).\n",
    "На чуть больше половины баллов за проект достаточно правильно сделать CharCNN-BLSTM-CRF для NER и вариант модели Miwa & Bansal (2016) для RE.\n",
    "Дедлайн 1 мая 9 утра (4 недели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9221d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brat_format\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bbaba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_types = {\"OUT\", \"ACT\", \"BIN\", \"CMP\", \"ECO\", \"INST\", \"MET\", \"SOC\", \"QUA\"}\n",
    "# rel_types = {\"NNG\", \"NNT\", \"NPS\", \"FNG\", \"FNT\", \"FPS\", \"PNG\", \"PNT\", \"PPS\", \"GOL\", \"TSK\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9551b",
   "metadata": {},
   "source": [
    "## Читаю данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ad6ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(ann_file):\n",
    "    br_doc = brat_format.read_file(ann_file)\n",
    "    segs = sent_tokenize(br_doc.txt_data, language=\"russian\")\n",
    "\n",
    "    count_chars = 0\n",
    "    cnt_global_ners = 0\n",
    "    ners_text = []\n",
    "    for i in range(len(segs)):\n",
    "        seg = segs[i]\n",
    "\n",
    "        while br_doc.txt_data[count_chars] != seg[0]:\n",
    "            count_chars += 1\n",
    "\n",
    "        end_seg = count_chars + len(seg)\n",
    "        ners_this_seg = []\n",
    "        while br_doc.ners and br_doc.ners[0][1] <= end_seg:\n",
    "            ent = br_doc.ners.pop(0)\n",
    "            #print(ent)\n",
    "            ners_this_seg.append((ent[0] - count_chars, ent[1] - count_chars, ent[2]))\n",
    "\n",
    "        count_chars += len(seg)\n",
    "        ners_text.append(ners_this_seg)\n",
    "\n",
    "    #     print('-' * 20, 'NER', '-' * 20)\n",
    "    #     for ner in ners_this_seg:\n",
    "    #         print(ner[0], ner[1], seg[ner[0]: ner[1]], ner[2])\n",
    "\n",
    "    #     print('\\n\\n\\n')\n",
    "    \n",
    "    return segs, ners_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45b098",
   "metadata": {},
   "source": [
    "## READ TRAIN_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08b6b648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10631 10631\n"
     ]
    }
   ],
   "source": [
    "train_val_texts, train_val_ners = [], [] \n",
    "\n",
    "for i in range(1, 4):\n",
    "    PATH = f'train_data/train_part_{i}/train_part_{i}/'\n",
    "    texts = []\n",
    "    ners = []\n",
    "\n",
    "    list_files = os.listdir(PATH)\n",
    "    ann_files = [x for x in list_files if x.endswith('ann')]\n",
    "    \n",
    "    for ann_file in ann_files:\n",
    "        segs, ners_file = read_data(PATH + ann_file)\n",
    "        \n",
    "        texts += segs\n",
    "        ners += ners_file\n",
    "    \n",
    "    train_val_texts += texts\n",
    "    train_val_ners += ners\n",
    "    \n",
    "print(len(train_val_texts), len(train_val_ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06744a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295 \n",
      "\n",
      " Всего за 2015 год в районе:\n",
      " - введено   в эксплуатацию  - 18544 кв.м. площади, в т.ч. \n",
      "- 11090 кв. м. жилой, \n",
      "- выдано 153 разрешения на строительство, реконструкцию, капитальный ремонт объектов социальной сферы, жилых домов, газовых и электрических сетей,\n",
      "- введено в эксплуатацию 50 объектов. \n",
      "\n",
      "введено   в эксплуатацию BIN\n",
      "выдано BIN\n",
      "разрешения на строительство, реконструкцию, капитальный ремонт объектов социальной сферы, жилых домов, газовых и электрических сетей ACT\n",
      "введено в эксплуатацию BIN\n"
     ]
    }
   ],
   "source": [
    "i = 343\n",
    "\n",
    "print(len(train_val_texts[i]), '\\n\\n', train_val_texts[i], '\\n')\n",
    "\n",
    "for ner in train_val_ners[i]:\n",
    "    print(train_val_texts[i][ner[0]: ner[1]], ner[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52a56c",
   "metadata": {},
   "source": [
    "## READ TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78bac4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19473 19473\n"
     ]
    }
   ],
   "source": [
    "PATH = 'test_data/test_ner_only/'\n",
    "\n",
    "list_files = os.listdir(PATH)\n",
    "ann_files = [x for x in list_files if x.endswith('ann')]\n",
    "\n",
    "test_texts = []\n",
    "test_ners = []\n",
    "for ann_file in ann_files:\n",
    "    segs, ners_text = read_data(PATH + ann_file)\n",
    "\n",
    "    test_texts += segs\n",
    "    test_ners += ners_text\n",
    "    \n",
    "print(len(test_texts), len(test_ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41a84bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CMP', 'ECO', 'ACT', 'BIN', 'QUA', 'MET', 'INST', 'SOC'}\n",
      "{'OUT'}\n"
     ]
    }
   ],
   "source": [
    "train_val_ners_exist = set()\n",
    "\n",
    "for spans in train_val_ners:\n",
    "    for span in spans:\n",
    "        train_val_ners_exist.add(span[2])\n",
    "        \n",
    "print(train_val_ners_exist)\n",
    "print(ner_types - train_val_ners_exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b398d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ECO', 'CMP', 'ACT', 'BIN', 'QUA', 'MET', 'INST', 'SOC'}\n",
      "{'OUT'}\n"
     ]
    }
   ],
   "source": [
    "test_ners_exist = set()\n",
    "\n",
    "for spans in test_ners:\n",
    "    for span in spans:\n",
    "        test_ners_exist.add(span[2])\n",
    "        \n",
    "print(test_ners_exist)\n",
    "print(ner_types - test_ners_exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988b000",
   "metadata": {},
   "source": [
    "## Labels to label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77d4f757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECO': 1,\n",
       " 'CMP': 2,\n",
       " 'ACT': 3,\n",
       " 'BIN': 4,\n",
       " 'QUA': 5,\n",
       " 'MET': 6,\n",
       " 'INST': 7,\n",
       " 'SOC': 8,\n",
       " 'OUT': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_2_tag_id = {v: k + 1 for k, v in enumerate(ner_types - {'OUT'})}\n",
    "tag_2_tag_id['OUT'] = 0\n",
    "\n",
    "tag_id_2_tag = {v: k for k, v in tag_2_tag_id.items()}\n",
    "\n",
    "tag_2_tag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40037a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from collections import namedtuple\n",
    "\n",
    "from razdel import tokenize\n",
    "\n",
    "from ipymarkup import show_box_markup\n",
    "from ipymarkup.palette import palette\n",
    "\n",
    "class Label(IntEnum):\n",
    "    OUTER = 0\n",
    "    BEGIN = 1\n",
    "    INNER = 2\n",
    "\n",
    "Sample = namedtuple(\"Sample\", \"text,tokens,spans,labels\")\n",
    "\n",
    "def text_span_to_sample(text, spans):\n",
    "    labels = []\n",
    "    tokens = list(tokenize(text))\n",
    "    \n",
    "    spans_copy = spans.copy()\n",
    "    for token in tokens:\n",
    "        label = tag_2_tag_id['OUT']\n",
    "        \n",
    "        if spans_copy:\n",
    "            span_begin, span_end, tag = spans_copy[0]\n",
    "\n",
    "            if token.start == span_begin or (token.start > span_begin and token.stop <= span_end):\n",
    "                label = tag_2_tag_id[tag]\n",
    "\n",
    "            if token.stop == span_end:\n",
    "                spans_copy.pop(0)\n",
    "            \n",
    "        labels.append(label)\n",
    "    \n",
    "    return Sample(text, tokens, spans, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "480770e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "train_val = []\n",
    "for text, spans in zip(train_val_texts, train_val_ners):\n",
    "    train_val.append(text_span_to_sample(text, spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c5d8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test = []\n",
    "for text, spans in zip(test_texts, test_ners):\n",
    "    test.append(text_span_to_sample(text, spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "acca3b2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tex2jax_ignore\" style=\"white-space: pre-wrap\">Глава <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">администрации<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">INST</span></span> \n",
       "муниципального района В.В. Бахвалов \n",
       "Приложение \n",
       "<span style=\"padding: 2px; border-radius: 4px; border: 1px solid #c8e6c9; background: #e8f5e9\">Утверждена<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #66bb6a;\">BIN</span></span> \n",
       "постановлением <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">администрации<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">INST</span></span> \n",
       "Чухломского муниципального района \n",
       "Костромской области \n",
       "от « 21 » июля 2017 г .</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "10631\n"
     ]
    }
   ],
   "source": [
    "i = 9346\n",
    "\n",
    "show_box_markup(train_val[i].text, train_val[i].spans)\n",
    "print(train_val[i].labels)\n",
    "print(len(train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99cd33da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tex2jax_ignore\" style=\"white-space: pre-wrap\">Основные итоги исполнения консолидированного\n",
       "<span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffcdd2; background: #ffebee\">бюджета<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #e57373;\">ECO</span></span> Владимирской <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffcdd2; background: #ffebee\">области<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #e57373;\">ECO</span></span> и <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffcdd2; background: #ffebee\">бюджета<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #e57373;\">ECO</span></span> территориального\n",
       "фонда обязательного медицинского страхования\n",
       "Владимирской области в 2016 году\n",
       "\n",
       "В 2016 году <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffcdd2; background: #ffebee\">мобилизовано<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #e57373;\">ECO</span></span> <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffe0b2; background: #fff3e0\">доходов<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #ffb74d;\">MET</span></span> в <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffe0b2; background: #fff3e0\">консолидированный<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #ffb74d;\">MET</span></span> <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffcdd2; background: #ffebee\">бюджет области<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #e57373;\">ECO</span></span> в сумме 61,9 млрд. рублей, или 102,1% к плану года и 107,4% к уровню 2015 года.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 6, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "19473\n"
     ]
    }
   ],
   "source": [
    "i = 7\n",
    "\n",
    "show_box_markup(test[i].text, test[i].spans)\n",
    "print(test[i].labels)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b262b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10631"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbf8ebbc",
   "metadata": {
    "id": "IwyMhmAaE_CY"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_val)\n",
    "\n",
    "train = train_val[:9500]\n",
    "val = train_val[9500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f905538",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JEWpoWt6CkP1",
    "outputId": "2851693e-8c5f-4d98-950e-f5afb74b664d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', 'щ', 'p', '+', 'V', 'n', 'е', 'T', 'ы', 'ф', 'K', 'Ё', 'Д', 'G', '7', 'M', '│', 'о', '2', 'Ж', '3', 'н', '8', ':', '5', 'ъ', 'у', '«', ']', 'Ч', 'j', 'и', '4', 'Т', 'z', 'м', 'Ы', 'u', 'ш', '\\uf0b7', '■', 'f', 'Э', 'S', '-', '{', '┌', 'Е', 'Л', 'g', 'Я', 'Р', 'Н', 'с', 'i', 'Ш', '»', '0', 'Ъ', 'F', '•', 'з', 'А', 'Ц', 'v', 'H', 'л', '}', 'п', 'd', '┬', '!', '>', 'В', 'x', 'y', 's', 'г', 'X', '‰', 'Σ', 'ә', 'к', '.', '─', 'в', '└', 'm', 'С', ',', 'ь', '=', 'e', 'Б', 'ж', 'L', 'э', 'К', 'б', '”', '×', '–', 'О', 'P', '%', 'k', 'R', 'o', 'җ', '∑', 'ё', 't', '…', '\"', ';', '@', 'х', '├', '6', 'Й', 'h', '№', 'З', 'b', 'р', '/', 'а', 'N', 'I', ')', 'Щ', 'П', 'Ю', 'A', 'М', 'Ә', '1', 'C', 'c', 'U', 'a', 'ч', 'Ö', 'т', '(', 'w', 'ю', '<', 'я', 'l', 'У', '[', 'O', '┘', '—', '┼', '“', '9', 'Х', 'D', '┤', 'д', 'Z', 'E', 'И', 'І', '┐', 'r', 'Г', '_', '£', 'Ф', 'й', 'Ь', 'ц', '*', 'B']\n"
     ]
    }
   ],
   "source": [
    "char_set = [\"<pad>\", \"<unk>\"] + list({ch for sample in train for token in sample.tokens for ch in token.text})\n",
    "print(char_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc205fe8",
   "metadata": {
    "id": "Bl0XXI9_iNJQ"
   },
   "source": [
    "Для каждого слова сохраняем его символьный состав, а в остальном старый добрый пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0c59e320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10631.000000\n",
       "mean       231.197065\n",
       "std        286.775179\n",
       "min          1.000000\n",
       "0%           1.000000\n",
       "10%         10.000000\n",
       "20%         73.000000\n",
       "30%        105.000000\n",
       "40%        134.000000\n",
       "50%        166.000000\n",
       "60%        202.000000\n",
       "70%        248.000000\n",
       "80%        320.000000\n",
       "90%        461.000000\n",
       "max       7562.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat = []\n",
    "\n",
    "for i in range(len(train_val)):\n",
    "    stat.append(len(train_val[i].text))\n",
    "\n",
    "pd.Series(stat).describe(percentiles=[x/10 for x in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4dcef6f8",
   "metadata": {
    "id": "ZBb2Ek8PmtJm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "class NerCharsDataset(Dataset):\n",
    "    def __init__(self, samples, char_set, max_seq_len=500, max_char_seq_len=50):\n",
    "        assert len(samples) != 0\n",
    "        self.samples = []\n",
    "        self.tokens = []\n",
    "        self.texts = []\n",
    "        for sample in samples:\n",
    "            inputs = torch.zeros((max_seq_len, max_char_seq_len), dtype=torch.long)\n",
    "            for token_num, token in enumerate(sample.tokens[:max_seq_len]):\n",
    "                for ch_num, ch in enumerate(token.text[:max_char_seq_len]):\n",
    "                    char_index = char_set.index(ch) if ch in char_set else char_set.index(\"<unk>\")\n",
    "                    inputs[token_num][ch_num] = char_index\n",
    "            labels = torch.zeros((max_seq_len,), dtype=torch.long)\n",
    "            input_labels = [int(i) for i in sample.labels[:max_seq_len]]\n",
    "            labels[:len(input_labels)] = torch.LongTensor(input_labels)\n",
    "            self.samples.append((torch.LongTensor(inputs), torch.LongTensor(labels)))\n",
    "            self.tokens.append(sample.tokens[:max_seq_len])\n",
    "            self.texts.append(sample.text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "\n",
    "    def get_tokens(self, index):\n",
    "        return self.tokens[index]\n",
    "    \n",
    "    def get_text(self, index):\n",
    "        return self.texts[index]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data = NerCharsDataset(train, char_set)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "\n",
    "val_data = NerCharsDataset(val, char_set)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = NerCharsDataset(test, char_set)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "75932023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 500, 50])\n",
      "torch.Size([64, 500])\n"
     ]
    }
   ],
   "source": [
    "for sample in train_loader:\n",
    "    inputs, labels = sample\n",
    "    print(inputs.size())\n",
    "    print(labels.size())\n",
    "    break\n",
    "\n",
    "# inputs: batch_size x num_words x num_chars\n",
    "# labels: batch_size x num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dd9d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_classes = torch.Tensor([0.3] + [1.0 for _ in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "495307ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "\n",
    "from torchmetrics import F1Score\n",
    "from TorchCRF import CRF\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    " \n",
    "\n",
    "class TemplateModel(LightningModule):\n",
    "    def __init__(self, classes_count=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.valid_fmera = F1Score(task=\"multiclass\", num_classes=9, average='macro', mdmc_average='global')\n",
    "        self.test_fmera = F1Score(task=\"multiclass\", num_classes=9, average='macro', mdmc_average='global')\n",
    "#         self.valid_accuracy = Accuracy()\n",
    "#         self.test_accuracy = Accuracy()\n",
    "    \n",
    "    def forward(self, inputs, labels):\n",
    "        raise NotImplementedError(\"forward not implemented\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        return [optimizer]\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        inputs, labels = batch\n",
    "        loss, logits = self(inputs, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, _):\n",
    "        inputs, labels = batch\n",
    "        val_loss, logits = self(inputs, labels)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        \n",
    "        predicted_labels = logits.argmax(dim=2)\n",
    "        vf = self.valid_fmera(predicted_labels, labels)\n",
    "        self.log(\"val_fmera\", vf.item(), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "#         self.valid_accuracy.update(logits, labels)\n",
    "#         self.log(\"val_acc\", self.valid_accuracy)\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        print('')\n",
    "        #self.log(\"val_fmera_epoch\", self.valid_fmera.compute(), prog_bar=True)\n",
    "        #self.log(\"val_acc_epoch\", self.valid_accuracy.compute(), prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        inputs, labels = batch\n",
    "        test_loss, logits = self(inputs, labels)\n",
    "        self.log(\"test_loss\", test_loss, prog_bar=True)\n",
    "        \n",
    "        predicted_labels = logits.argmax(dim=2)\n",
    "        tf = self.test_fmera(predicted_labels, labels)\n",
    "        self.log(\"test_fmera\", tf.item(), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "#         self.test_accuracy.update(logits, labels)\n",
    "#         self.log(\"test_acc\", self.test_accuracy)\n",
    "\n",
    "    def test_epoch_end(self, outs):\n",
    "        print('')\n",
    "        self.log(\"test_fmera_epoch\", self.test_fmera.compute(), prog_bar=True)\n",
    "        #self.log(\"test_acc_epoch\", self.test_accuracy.compute(), prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "546b46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharFFLstmModel(TemplateModel):\n",
    "    def __init__(self, char_set_size, weights_classes, char_embedding_dim=8, classes_count=9,\n",
    "                 word_embedding_dim=32, lstm_embedding_dim=32, char_max_seq_len=50):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_layer = nn.Embedding(char_set_size, char_embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(char_embedding_dim * char_max_seq_len, word_embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm_layer = nn.LSTM(word_embedding_dim, lstm_embedding_dim // 2, batch_first=True, bidirectional=True)\n",
    "        self.out_layer = nn.Linear(lstm_embedding_dim, classes_count)\n",
    "        #self.loss = nn.CrossEntropyLoss(weight=weights_classes)\n",
    "        self.loss = CRF(classes_count, batch_first=True)\n",
    "        #self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        projections = self.embeddings_layer.forward(inputs)\n",
    "        projections = projections.reshape(projections.size(0), projections.size(1), -1)\n",
    "        projections = self.relu(self.linear(projections))\n",
    "        projections = self.dropout(projections)\n",
    "        output, _= self.lstm_layer(projections)\n",
    "        output = self.dropout(output)\n",
    "        logits = self.out_layer.forward(output)\n",
    "        #logits = self.softmax(logits)\n",
    "        #logits = logits.transpose(1, 2)\n",
    "        loss = -self.loss(logits, labels)\n",
    "        \n",
    "        return loss, logits\n",
    "    \n",
    "char_ff_lstm_model = CharFFLstmModel(len(char_set), weights_classes, classes_count=len(ner_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ebf87015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name             | Type      | Params\n",
      "-----------------------------------------------\n",
      "0 | loss             | CRF       | 99    \n",
      "1 | valid_fmera      | F1Score   | 0     \n",
      "2 | test_fmera       | F1Score   | 0     \n",
      "3 | embeddings_layer | Embedding | 1.4 K \n",
      "4 | dropout          | Dropout   | 0     \n",
      "5 | linear           | Linear    | 12.8 K\n",
      "6 | relu             | ReLU      | 0     \n",
      "7 | lstm_layer       | LSTM      | 6.4 K \n",
      "8 | out_layer        | Linear    | 297   \n",
      "-----------------------------------------------\n",
      "21.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.1 K    Total params\n",
      "0.084     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|                                                                   | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:  90%|████▍| 150/167 [01:32<00:10,  1.63it/s, loss=4.18e+03, v_num=40, val_loss=6.55e+4, val_fmera_epoch=0.110]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  96%|████▊| 160/167 [01:34<00:04,  1.70it/s, loss=4.18e+03, v_num=40, val_loss=6.55e+4, val_fmera_epoch=0.110]\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.90it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 167/167 [01:35<00:00,  1.74it/s, loss=3.95e+03, v_num=40, val_loss=3.84e+3, val_fmera_epoch=0.110, val\n",
      "Epoch 1:  90%|▉| 150/167 [01:30<00:10,  1.66it/s, loss=1.93e+03, v_num=40, val_loss=3.84e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.93e+03, v_num=40, val_loss=3.84e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.97it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.86e+03, v_num=40, val_loss=1.73e+3, val_fmera_epoch=0.110, val\n",
      "Epoch 2:  90%|▉| 150/167 [01:30<00:10,  1.65it/s, loss=1.85e+03, v_num=40, val_loss=1.73e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  96%|▉| 160/167 [01:32<00:04,  1.72it/s, loss=1.85e+03, v_num=40, val_loss=1.73e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.94it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.76e+03, v_num=40, val_loss=1.57e+3, val_fmera_epoch=0.110, val\n",
      "Epoch 3:  90%|▉| 150/167 [01:31<00:10,  1.65it/s, loss=1.66e+03, v_num=40, val_loss=1.57e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  96%|▉| 160/167 [01:33<00:04,  1.72it/s, loss=1.66e+03, v_num=40, val_loss=1.57e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.87it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 167/167 [01:34<00:00,  1.76it/s, loss=1.6e+03, v_num=40, val_loss=1.51e+3, val_fmera_epoch=0.110, val_\n",
      "Epoch 4:  90%|▉| 150/167 [01:30<00:10,  1.65it/s, loss=1.6e+03, v_num=40, val_loss=1.51e+3, val_fmera_epoch=0.110, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  96%|▉| 160/167 [01:32<00:04,  1.72it/s, loss=1.6e+03, v_num=40, val_loss=1.51e+3, val_fmera_epoch=0.110, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.91it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.56e+03, v_num=40, val_loss=1.45e+3, val_fmera_epoch=0.110, val\n",
      "Epoch 5:  90%|▉| 150/167 [01:30<00:10,  1.66it/s, loss=1.62e+03, v_num=40, val_loss=1.45e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.62e+03, v_num=40, val_loss=1.45e+3, val_fmera_epoch=0.110, val\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.93it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.48e+03, v_num=40, val_loss=1.36e+3, val_fmera_epoch=0.146, val\n",
      "Epoch 6:  90%|▉| 150/167 [01:30<00:10,  1.66it/s, loss=1.45e+03, v_num=40, val_loss=1.36e+3, val_fmera_epoch=0.146, val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.45e+03, v_num=40, val_loss=1.36e+3, val_fmera_epoch=0.146, val\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.91it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.41e+03, v_num=40, val_loss=1.29e+3, val_fmera_epoch=0.161, val\n",
      "Epoch 7:  90%|▉| 150/167 [01:30<00:10,  1.66it/s, loss=1.37e+03, v_num=40, val_loss=1.29e+3, val_fmera_epoch=0.161, val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.37e+03, v_num=40, val_loss=1.29e+3, val_fmera_epoch=0.161, val\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.92it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.33e+03, v_num=40, val_loss=1.23e+3, val_fmera_epoch=0.176, val\n",
      "Epoch 8:  90%|▉| 150/167 [01:30<00:10,  1.65it/s, loss=1.35e+03, v_num=40, val_loss=1.23e+3, val_fmera_epoch=0.176, val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  96%|▉| 160/167 [01:32<00:04,  1.72it/s, loss=1.35e+03, v_num=40, val_loss=1.23e+3, val_fmera_epoch=0.176, val\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.98it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.32e+03, v_num=40, val_loss=1.2e+3, val_fmera_epoch=0.189, val_\n",
      "Epoch 9:  90%|▉| 150/167 [01:30<00:10,  1.66it/s, loss=1.29e+03, v_num=40, val_loss=1.2e+3, val_fmera_epoch=0.189, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.29e+03, v_num=40, val_loss=1.2e+3, val_fmera_epoch=0.189, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.93it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.21e+03, v_num=40, val_loss=1.17e+3, val_fmera_epoch=0.200, val\n",
      "Epoch 10:  90%|▉| 150/167 [01:30<00:10,  1.65it/s, loss=1.29e+03, v_num=40, val_loss=1.17e+3, val_fmera_epoch=0.200, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.29e+03, v_num=40, val_loss=1.17e+3, val_fmera_epoch=0.200, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.80it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.24e+03, v_num=40, val_loss=1.09e+3, val_fmera_epoch=0.265, va\n",
      "Epoch 11:  90%|▉| 150/167 [01:30<00:10,  1.66it/s, loss=1.23e+03, v_num=40, val_loss=1.09e+3, val_fmera_epoch=0.265, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.23e+03, v_num=40, val_loss=1.09e+3, val_fmera_epoch=0.265, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.89it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.26e+03, v_num=40, val_loss=1.06e+3, val_fmera_epoch=0.284, va\n",
      "Epoch 12:  90%|▉| 150/167 [01:30<00:10,  1.65it/s, loss=1.23e+03, v_num=40, val_loss=1.06e+3, val_fmera_epoch=0.284, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  96%|▉| 160/167 [01:32<00:04,  1.73it/s, loss=1.23e+03, v_num=40, val_loss=1.06e+3, val_fmera_epoch=0.284, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.98it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 167/167 [01:34<00:00,  1.77it/s, loss=1.13e+03, v_num=40, val_loss=1.05e+3, val_fmera_epoch=0.317, va\n",
      "Epoch 13:  90%|▉| 150/167 [01:31<00:10,  1.65it/s, loss=1.15e+03, v_num=40, val_loss=1.05e+3, val_fmera_epoch=0.317, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  96%|▉| 160/167 [01:33<00:04,  1.72it/s, loss=1.15e+03, v_num=40, val_loss=1.05e+3, val_fmera_epoch=0.317, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.94it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 167/167 [01:34<00:00,  1.76it/s, loss=1.17e+03, v_num=40, val_loss=1.03e+3, val_fmera_epoch=0.331, va\n",
      "Epoch 14:  90%|▉| 150/167 [01:32<00:10,  1.62it/s, loss=1.17e+03, v_num=40, val_loss=1.03e+3, val_fmera_epoch=0.331, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  96%|▉| 160/167 [01:34<00:04,  1.69it/s, loss=1.17e+03, v_num=40, val_loss=1.03e+3, val_fmera_epoch=0.331, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.75it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 167/167 [01:36<00:00,  1.73it/s, loss=1.13e+03, v_num=40, val_loss=1.01e+3, val_fmera_epoch=0.326, va\n",
      "Epoch 15:  90%|▉| 150/167 [01:34<00:10,  1.60it/s, loss=1.15e+03, v_num=40, val_loss=1.01e+3, val_fmera_epoch=0.326, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  96%|▉| 160/167 [01:36<00:04,  1.66it/s, loss=1.15e+03, v_num=40, val_loss=1.01e+3, val_fmera_epoch=0.326, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.78it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 167/167 [01:37<00:00,  1.71it/s, loss=1.1e+03, v_num=40, val_loss=991.0, val_fmera_epoch=0.355, val_f\n",
      "Epoch 16:  90%|▉| 150/167 [01:35<00:10,  1.57it/s, loss=1.13e+03, v_num=40, val_loss=991.0, val_fmera_epoch=0.355, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  96%|▉| 160/167 [01:37<00:04,  1.64it/s, loss=1.13e+03, v_num=40, val_loss=991.0, val_fmera_epoch=0.355, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.53it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 167/167 [01:39<00:00,  1.68it/s, loss=1.08e+03, v_num=40, val_loss=980.0, val_fmera_epoch=0.349, val_\n",
      "Epoch 17:  90%|▉| 150/167 [01:31<00:10,  1.64it/s, loss=1.12e+03, v_num=40, val_loss=980.0, val_fmera_epoch=0.349, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  96%|▉| 160/167 [01:33<00:04,  1.71it/s, loss=1.12e+03, v_num=40, val_loss=980.0, val_fmera_epoch=0.349, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.94it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 167/167 [01:35<00:00,  1.75it/s, loss=1.09e+03, v_num=40, val_loss=984.0, val_fmera_epoch=0.368, val_\n",
      "Epoch 18:  90%|▉| 150/167 [01:29<00:10,  1.68it/s, loss=1.06e+03, v_num=40, val_loss=984.0, val_fmera_epoch=0.368, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  96%|▉| 160/167 [01:31<00:03,  1.75it/s, loss=1.06e+03, v_num=40, val_loss=984.0, val_fmera_epoch=0.368, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.96it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 167/167 [01:32<00:00,  1.80it/s, loss=1.06e+03, v_num=40, val_loss=963.0, val_fmera_epoch=0.366, val_\n",
      "Epoch 19:  90%|▉| 150/167 [01:29<00:10,  1.68it/s, loss=1.13e+03, v_num=40, val_loss=963.0, val_fmera_epoch=0.366, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  96%|▉| 160/167 [01:31<00:03,  1.75it/s, loss=1.13e+03, v_num=40, val_loss=963.0, val_fmera_epoch=0.366, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.84it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 167/167 [01:32<00:00,  1.80it/s, loss=1.13e+03, v_num=40, val_loss=964.0, val_fmera_epoch=0.369, val_\n",
      "Epoch 20:  90%|▉| 150/167 [01:33<00:10,  1.61it/s, loss=1.17e+03, v_num=40, val_loss=964.0, val_fmera_epoch=0.369, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  96%|▉| 160/167 [01:35<00:04,  1.68it/s, loss=1.17e+03, v_num=40, val_loss=964.0, val_fmera_epoch=0.369, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.67it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 167/167 [01:36<00:00,  1.72it/s, loss=1.14e+03, v_num=40, val_loss=949.0, val_fmera_epoch=0.369, val_\n",
      "Epoch 21:  90%|▉| 150/167 [01:34<00:10,  1.59it/s, loss=1.13e+03, v_num=40, val_loss=949.0, val_fmera_epoch=0.369, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  96%|▉| 160/167 [01:36<00:04,  1.65it/s, loss=1.13e+03, v_num=40, val_loss=949.0, val_fmera_epoch=0.369, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.78it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 167/167 [01:38<00:00,  1.70it/s, loss=1.09e+03, v_num=40, val_loss=957.0, val_fmera_epoch=0.386, val_\n",
      "Epoch 22:  90%|▉| 150/167 [01:31<00:10,  1.64it/s, loss=1.11e+03, v_num=40, val_loss=957.0, val_fmera_epoch=0.386, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  96%|▉| 160/167 [01:33<00:04,  1.70it/s, loss=1.11e+03, v_num=40, val_loss=957.0, val_fmera_epoch=0.386, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.61it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 167/167 [01:35<00:00,  1.75it/s, loss=1.12e+03, v_num=40, val_loss=962.0, val_fmera_epoch=0.388, val_\n",
      "Epoch 23:  90%|▉| 150/167 [01:33<00:10,  1.60it/s, loss=1.14e+03, v_num=40, val_loss=962.0, val_fmera_epoch=0.388, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  96%|▉| 160/167 [01:35<00:04,  1.67it/s, loss=1.14e+03, v_num=40, val_loss=962.0, val_fmera_epoch=0.388, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.69it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 167/167 [01:37<00:00,  1.71it/s, loss=1.14e+03, v_num=40, val_loss=932.0, val_fmera_epoch=0.384, val_\n",
      "Epoch 24:  90%|▉| 150/167 [01:34<00:10,  1.59it/s, loss=1.11e+03, v_num=40, val_loss=932.0, val_fmera_epoch=0.384, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  96%|▉| 160/167 [01:36<00:04,  1.66it/s, loss=1.11e+03, v_num=40, val_loss=932.0, val_fmera_epoch=0.384, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.76it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 167/167 [01:37<00:00,  1.70it/s, loss=1.1e+03, v_num=40, val_loss=932.0, val_fmera_epoch=0.388, val_f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  90%|▉| 150/167 [01:34<00:10,  1.58it/s, loss=1.13e+03, v_num=40, val_loss=932.0, val_fmera_epoch=0.388, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  96%|▉| 160/167 [01:37<00:04,  1.65it/s, loss=1.13e+03, v_num=40, val_loss=932.0, val_fmera_epoch=0.388, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.60it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 167/167 [01:38<00:00,  1.69it/s, loss=1.07e+03, v_num=40, val_loss=920.0, val_fmera_epoch=0.391, val_\n",
      "Epoch 26:  90%|▉| 150/167 [01:32<00:10,  1.62it/s, loss=1.04e+03, v_num=40, val_loss=920.0, val_fmera_epoch=0.391, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  96%|▉| 160/167 [01:34<00:04,  1.69it/s, loss=1.04e+03, v_num=40, val_loss=920.0, val_fmera_epoch=0.391, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.68it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 167/167 [01:36<00:00,  1.73it/s, loss=1.01e+03, v_num=40, val_loss=927.0, val_fmera_epoch=0.404, val_\n",
      "Epoch 27:  90%|▉| 150/167 [01:34<00:10,  1.59it/s, loss=1.15e+03, v_num=40, val_loss=927.0, val_fmera_epoch=0.404, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  96%|▉| 160/167 [01:36<00:04,  1.66it/s, loss=1.15e+03, v_num=40, val_loss=927.0, val_fmera_epoch=0.404, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.70it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 167/167 [01:37<00:00,  1.71it/s, loss=1.12e+03, v_num=40, val_loss=935.0, val_fmera_epoch=0.401, val_\n",
      "Epoch 28:  90%|▉| 150/167 [01:33<00:10,  1.61it/s, loss=1.13e+03, v_num=40, val_loss=935.0, val_fmera_epoch=0.401, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  96%|▉| 160/167 [01:35<00:04,  1.68it/s, loss=1.13e+03, v_num=40, val_loss=935.0, val_fmera_epoch=0.401, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.73it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 167/167 [01:36<00:00,  1.72it/s, loss=1.08e+03, v_num=40, val_loss=942.0, val_fmera_epoch=0.401, val_\n",
      "Epoch 29:  90%|▉| 150/167 [01:32<00:10,  1.61it/s, loss=1.17e+03, v_num=40, val_loss=942.0, val_fmera_epoch=0.401, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  96%|▉| 160/167 [01:35<00:04,  1.68it/s, loss=1.17e+03, v_num=40, val_loss=942.0, val_fmera_epoch=0.401, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.92it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 167/167 [01:36<00:00,  1.73it/s, loss=1.14e+03, v_num=40, val_loss=979.0, val_fmera_epoch=0.410, val_\n",
      "Epoch 30:  90%|▉| 150/167 [01:33<00:10,  1.60it/s, loss=1.17e+03, v_num=40, val_loss=979.0, val_fmera_epoch=0.410, val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  96%|▉| 160/167 [01:35<00:04,  1.67it/s, loss=1.17e+03, v_num=40, val_loss=979.0, val_fmera_epoch=0.410, val_\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.85it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 167/167 [01:37<00:00,  1.71it/s, loss=1.21e+03, v_num=40, val_loss=1.05e+3, val_fmera_epoch=0.403, va\n",
      "Epoch 31:  90%|▉| 150/167 [01:33<00:10,  1.61it/s, loss=1.48e+03, v_num=40, val_loss=1.05e+3, val_fmera_epoch=0.403, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  96%|▉| 160/167 [01:35<00:04,  1.68it/s, loss=1.48e+03, v_num=40, val_loss=1.05e+3, val_fmera_epoch=0.403, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.70it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 167/167 [01:37<00:00,  1.72it/s, loss=1.44e+03, v_num=40, val_loss=1.17e+3, val_fmera_epoch=0.373, va\n",
      "Epoch 32:  90%|▉| 150/167 [01:34<00:10,  1.59it/s, loss=1.48e+03, v_num=40, val_loss=1.17e+3, val_fmera_epoch=0.373, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  96%|▉| 160/167 [01:36<00:04,  1.66it/s, loss=1.48e+03, v_num=40, val_loss=1.17e+3, val_fmera_epoch=0.373, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.73it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 167/167 [01:37<00:00,  1.70it/s, loss=1.39e+03, v_num=40, val_loss=1.16e+3, val_fmera_epoch=0.373, va\n",
      "Epoch 33:  90%|▉| 150/167 [01:33<00:10,  1.61it/s, loss=1.52e+03, v_num=40, val_loss=1.16e+3, val_fmera_epoch=0.373, va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  96%|▉| 160/167 [01:35<00:04,  1.67it/s, loss=1.52e+03, v_num=40, val_loss=1.16e+3, val_fmera_epoch=0.373, va\u001b[A\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  4.72it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 167/167 [01:37<00:00,  1.72it/s, loss=1.45e+03, v_num=40, val_loss=1.19e+3, val_fmera_epoch=0.395, va\n",
      "Epoch 33: 100%|█| 167/167 [01:37<00:00,  1.72it/s, loss=1.45e+03, v_num=40, val_loss=1.19e+3, val_fmera_epoch=0.395, va\u001b[A\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 305/305 [01:06<00:00,  4.43it/s]\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'CharFFLstmModel' object has no attribute 'test_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-6848e84df5a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     callbacks=[early_stop_callback])\n\u001b[0;32m     16\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_ff_lstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_ff_lstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__test_given_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    923\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__test_using_best_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m__test_given_model\u001b[1;34m(self, model, test_dataloaders)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;31m# run test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[1;31m# sets up testing so we short circuit to eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;31m# teardown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mstart_testing\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mstart_testing\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# double dispatch to initiate the test loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;31m# self.reset_test_dataloader(ref_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_test_evaluation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m             \u001b[0meval_loop_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_loop_results\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[1;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;31m# lightning module method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m         \u001b[0mdeprecated_eval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[1;31m# hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_epoch_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;31m# call the model epoch end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mdeprecated_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__run_eval_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;31m# enable returning anything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\u001b[0m in \u001b[0;36m__run_eval_epoch_end\u001b[1;34m(self, num_dataloaders)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_overridden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_epoch_end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test_epoch_end'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m                 \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m                 \u001b[0muser_reduced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-ed9f473d473a>\u001b[0m in \u001b[0;36mtest_epoch_end\u001b[1;34m(self, outs)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_fmera_epoch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_fmera\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_acc_epoch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\sber_nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 779\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'CharFFLstmModel' object has no attribute 'test_accuracy'"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.0,\n",
    "    patience=8,\n",
    "    verbose=True,\n",
    "    mode=\"min\" \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=1,\n",
    "    checkpoint_callback=False,\n",
    "    accumulate_grad_batches=1,\n",
    "    max_epochs=200,\n",
    "    progress_bar_refresh_rate=10,\n",
    "    callbacks=[early_stop_callback])\n",
    "trainer.fit(char_ff_lstm_model, train_loader, val_loader)\n",
    "trainer.test(char_ff_lstm_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10a2ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(char_ff_lstm_model.state_dict(), 'model_3004_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9882fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692K\tmodel_2704.pth\n"
     ]
    }
   ],
   "source": [
    "! du -hs model_2704.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "553e66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_ff_lstm_model = CharFFLstmModel(len(char_set), weights_classes, classes_count=len(ner_types))\n",
    "# char_ff_lstm_model.load_state_dict(torch.load(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da175602",
   "metadata": {},
   "source": [
    "# Оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4657057",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 500\n",
    "\n",
    "test_sizz = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a7bfa85f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index:  0\n",
      "batch_index:  100\n",
      "batch_index:  200\n",
      "batch_index:  300\n"
     ]
    }
   ],
   "source": [
    "model = char_ff_lstm_model\n",
    "\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "\n",
    "global_true = []\n",
    "global_pred = []\n",
    "labels_pred = []\n",
    "\n",
    "for batch_index, batch in enumerate(test_loader):\n",
    "    #print(batch_index)\n",
    "    inputs, true_labels = batch\n",
    "    batch_size = inputs.size(0)\n",
    "    _, logits = model(inputs.to(\"cuda\"), true_labels.to(\"cuda\"))\n",
    "    \n",
    "    predicted_labels = logits.argmax(dim=2).detach().cpu()\n",
    "    labels_pred.append(predicted_labels)\n",
    "    \n",
    "    for i in range(BATCH_SIZE):\n",
    "        num = batch_index * BATCH_SIZE + i\n",
    "        if num == test_sizz:\n",
    "            break\n",
    "        \n",
    "        #print('text: ', i)\n",
    "        ners_model = []\n",
    "        \n",
    "        tokens = list(tokenize(test_texts[num]))\n",
    "        tokens = tokens[:MAX_SEQ_LEN]\n",
    "\n",
    "        indices = [i for i, x in enumerate(predicted_labels[i]) if x != 0]\n",
    "        indices_tensor = torch.Tensor(indices).type(torch.int64)\n",
    "\n",
    "        vals = predicted_labels[i].gather(dim=0, index=indices_tensor)\n",
    "        tokens_selected = [tokens[i] for i in indices]\n",
    "\n",
    "        pred_tag_id = None\n",
    "        start = None\n",
    "        stop = None\n",
    "        for token, val in zip(tokens_selected, vals):\n",
    "            tag_id = int(val.item())\n",
    "            \n",
    "            if tag_id == pred_tag_id and token.start == stop + 1:\n",
    "                stop = token.stop\n",
    "            else:\n",
    "                if pred_tag_id is not None:\n",
    "                    ners_model.append((start, stop, tag_id_2_tag[pred_tag_id]))\n",
    "                    \n",
    "                pred_tag_id = tag_id\n",
    "                start = token.start\n",
    "                stop = token.stop\n",
    "                \n",
    "        if (len(tokens_selected) != 0 and\n",
    "            (len(ners_model) == 0 or ners_model[-1][1] != stop)):\n",
    "            ners_model.append((start, stop, tag_id_2_tag[pred_tag_id]))\n",
    "            \n",
    "        global_true.append(test_ners[i])\n",
    "        global_pred.append(ners_model)\n",
    "\n",
    "        tp, fp, fn = evaluate_ners.cacl_ner_tp_fp_fn(test_ners[i], ners_model)\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        \n",
    "    if batch_index % 100 == 0:\n",
    "        print('batch_index: ', batch_index)\n",
    "        \n",
    "#     if batch_index == 1:\n",
    "#         break\n",
    "        \n",
    "precision, recall = evaluate_ners.compute_precision_and_recall(total_tp, total_fp, total_fn)\n",
    "f_measure = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e03a2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 152551, 61686)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tp, total_fp, total_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a8c7ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010364372469635627 0.00041935589555417227 0.0005971124017446879\n"
     ]
    }
   ],
   "source": [
    "precision, recall = evaluate_ners.compute_precision_and_recall(total_tp, total_fp, total_fn)\n",
    "f_measure = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(precision, recall, f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4963040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sber_emb",
   "language": "python",
   "name": "sber_emb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
